<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Miao Lu</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Homepage</div>
<div class="menu-item"><a href="index.html" class="current">About&nbsp;Me</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="experiences.html">Experiences</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Miao Lu</h1>
</div>
<table class="imgtable"><tr><td>
<img src="profile_seattle.png" alt="alt text" width="325px" height="325px" />&nbsp;</td>
<td align="left"><p><b>About Me</b><br /></p>
<p>Welcome to my homepage!</p>
<p>I am a Ph.D. candidate in Operations Research from <a href="https://msande.stanford.edu">the Department of Management Science and Engineering (MS&amp;E)</a> at <a href="https://www.stanford.edu">Stanford University</a>.
Prior to that, I obtained my Bachelor's degree in Probability and Statistics from the <a href="http://en.ustc.edu.cn">University of Science and Technology of China (USTC)</a>, where I won the Guo Moruo scholarship, the highest honor awarding undergraduates of USTC.</p>
<p><b>Research Interests</b><br /></p>
<p>My current research interest centers around reinforcement learning for LLM agents and training.</p>
<p>With backgrounds in probability and statistics, my past research includes mathematical theory and algorithm design of provably sample-efficient reinforcement learning, reinforcement learning from human feedback for LLM, training dynamics and generalization of optimization in deep learning landscapes, and reinforcement learning for operations and economics.</p>
<p><b>Contact Information</b><br /></p>
<p>E-mail addresses: miaolu@stanford.edu, lumiao@mail.ustc.edu.cn<br /></p>
<p>Here are my [<a href="https://scholar.google.com/citations?hl=en&amp;user=3jS17zQAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Google Scholar</a>] [<a href="https://github.com/MiaoLu3">GitHub</a>] [<a href="https://www.linkedin.com/in/miao-lu-5bb9a31aa">LinkedIn</a>] [<a href="Curriculum_Vitae.pdf">CV</a>]</p>
</td></tr></table>
<h2>Industrial Experiences </h2>
<p><b>ByteDance Seed</b>, San Jose, USA. <i>Jun. 2025 - Sep.2025</i> </p>
<ul>
<li><p>Researcher Scientist Intern in Foundations Models</p>
</li>
</ul>
<p><b>Ubiquant Investment</b>, Shanghai, China. <i>Jun. 2022 - Sep. 2022</i></p>
<ul>
<li><p>Quantitative Research Intern in the AI Department</p>
</li>
</ul>
<h2>Selected Publications  </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2510.06727">Scaling Multi-Turn LLM RL via End-to-End Summarization-based Context Management</a> <br /> 
with Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, Jiecao Chen <br /> 
<i>Submitted, Sep, 2025</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2405.16436">Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer</a><br /> 
with Zhihan Liu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, Zhaoran Wang <br /> 
<i>Neural Information Processing Systems (NeurIPS) 2024</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2310.17074">Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates</a><br /> 
with Beining Wu, Xiaodong Yang, Difan Zou <br /> 
<i>International Conference on Learning Representations (ICLR) 2024</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.18258">Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration</a><br /> 
with Zhihan Liu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang <br /> 
<i>Neural Information Processing Systems (NeurIPS) 2023</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.09659">Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</a><br /> 
with Jose Blanchet, Tong Zhang, Han Zhong <br /> 
<i>Neural Information Processing Systems (NeurIPS) 2023</i> <br /></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Copyright 2022 Miao Lu. Page generated 2025-10-09 01:15:52 PDT.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>

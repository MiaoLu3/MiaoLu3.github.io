<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Homepage</div>
<div class="menu-item"><a href="index.html">About&nbsp;Me</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="experiences.html">Experiences</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<p>Authors with * contributed equally.</p>
<h2>2025 </h2>
<ul>
<li><p><a href="https://openreview.net/forum?id=8V78zZDgFj">Can Neural Networks Achieve Optimal Computational-Statistical Tradeoff? An Analysis on Single-Index Model</a><br />
Siyu Chen*, Beining Wu*, <b>Miao Lu</b>, Zhuoran Yang, Tianhao Wang <br />
<i>NeurIPS Workshop on Mathematics of Modern Machine Learning (M3L) 2024</i> <b>Oral</b>  <br /> 
<i>International Conference on Learning Representations (ICLR) 2025</i> [<a href="M3L_slides.pdf"><b>Slides</b></a>] <br /></p>
</li>
</ul>
<h2>Before 2025</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2405.16436">Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer</a><br /> 
Zhihan Liu*, <b>Miao Lu*</b>, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, Zhaoran Wang <br /> 
<i>ICML Workshop on Aligning Reinforcement Learning Experimentalists and Theorists (ARLET) 2024</i> <br />
<i>Neural Information Processing Systems (NeurIPS) 2024</i> [<a href="ArXiv_RPO.pdf"><b>PDF</b></a>] [<a href="https://github.com/YSLIU627/Regularized-Preference-Optimization"><b>Code</b></a>] [<a href="Poster_RPO.pdf"><b>Poster</b></a>]<br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2404.03578">Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm</a><br /> 
<b>Miao Lu*</b>, Han Zhong*, Tong Zhang, Jose Blanchet <br /> 
<i>ICML Workshop on Aligning Reinforcement Learning Experimentalists and Theorists (ARLET) 2024</i> <br />
<i>Neural Information Processing Systems (NeurIPS) 2024</i> [<a href="ArXiv_DRRL_Interactive.pdf"><b>PDF</b></a>]  [<a href="NeurIPS2024IDCSlides.pdf"><b>Slides</b></a>]<br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2310.17074">Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates</a><br /> 
<b>Miao Lu*</b>, Beining Wu*, Xiaodong Yang, Difan Zou <br /> 
<i>NeurIPS Workshop on Mathematics of Modern Machine Learning (M3L) 2023</i> <br />
<i>International Conference on Learning Representations (ICLR) 2024</i> [<a href="ArXiv_Benign_Oscillation.pdf"><b>PDF</b></a>] [<a href="Benign_Oscillation_Poster.pdf"><b>Poster</b></a>] <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.18258">Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration</a><br /> 
Zhihan Liu*, <b>Miao Lu*</b>, Wei Xiong*, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang <br /> 
<i>Neural Information Processing Systems (NeurIPS) 2023</i> <b>Spotlight</b>  [<a href="ArXiv_MEX.pdf"><b>PDF</b></a>] [<a href="https://github.com/agentification/MEX"><b>Code</b></a>] [<a href="NeurIPS2023MEX_Slides.pdf"><b>Slides</b></a>]<br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.09659">Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</a><br /> 
Jose Blanchet, <b>Miao Lu</b>, Tong Zhang, Han Zhong (alphabetical) <br /> 
<i>Neural Information Processing Systems (NeurIPS) 2023</i> (short version)<br />
<i>Mathematics of Operations Research</i> (Major revision) (long version) [<a href="ArXiv_Double_Pessimism.pdf"><b>PDF</b></a>] [<a href="NeurIPS2023DoublePessimism_Slides.pdf"><b>Slides</b></a>]<br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.13589">Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes</a><br />
<b>Miao Lu</b>, Yifei Min, Zhaoran Wang, Zhuoran Yang <br />
<i>International Conference on Learning Representations (ICLR) 2023</i>  [<a href="ArXiv_Pessimism_Confounded_POMDP.pdf"><b>PDF</b></a>] [<a href="informs2022.pdf"><b>Slides</b></a>]<br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.mlr.press/v162/liu22l.html">Welfare Maximization in Competitive Equilibrium: Reinforcement Learning for Markov Exchange Economy</a><br />
Zhihan Liu*, <b>Miao Lu*</b>, Zhaoran Wang, Michael I. Jordan, Zhuoran Yang <br />
<i>International Conference on Machine Learning (ICML) 2022</i> [<a href="ICML2022RLMEE.pdf"><b>PDF</b></a>] [<a href="https://github.com/YSLIU627/RL-for-Markov-Exchange-Economy"><b>Code</b></a>] [<a href="ICML2022RLMEE_Slides.pdf"><b>Slides</b></a>]<br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=O1DEtITim__">Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, and No Retraining</a><br />
<b>Miao Lu*</b>, Xiaolong Luo*, Tianlong Chen, Wuyang Chen, Dong Liu, Zhangyang Wang <br />
<i>International Conference on Learning Representations (ICLR) 2022</i> <b>Spotlight</b> [<a href="ICLR2022SFWPruning.pdf"><b>PDF</b></a>] [<a href="https://github.com/VITA-Group/SFW-Once-for-All-Pruning"><b>Code</b></a>]  [<a href="ICLR2022SFWPruning_Slides.pdf"><b>Slides</b></a>]<br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.10513">Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization</a><br />
Yufei Kuang, <b>Miao Lu</b>, Jie Wang, Qi Zhou, Bin Li, Houqiang Li <br />
<i>Association for the Advancement of Artificial Intelligence (AAAI) 2022</i> [<a href="AAAI2022SCPO.pdf"><b>PDF</b></a>] [<a href="https://github.com/MIRALab-USTC/RL-SCPO"><b>Code</b></a>] [<a href="AAAI2022SCPO_Slides.pdf"><b>Slides</b></a>]</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Copyright 2022 Miao Lu. Page generated 2025-01-22 14:40:52 PST.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
